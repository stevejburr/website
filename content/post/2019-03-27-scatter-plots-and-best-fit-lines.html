---
title: Scatter plots and best fit lines
author: Steve Burr
date: '2019-03-27'
slug: scatter-plots-and-best-fit-lines
categories:
  - Statistics
  - R
  - DataViz
  - ggplot2
tags: []
---



<p>Last week I came across an interesting conversation on twitter, which raiased something I hadn’t thought about before:</p>
{{% tweet "1107680006476578818" %}}
<p>There were a number of good responses, including links to a few blog posts digging into how all this works, but I wanted to try and summarise what I think are the most important points and have a go at putting together my own versions of the visualisations.</p>
<div id="the-basics" class="section level4">
<h4>The basics</h4>
<p>We’re going to need some simulated data to plot, so let’s simulate some using R:</p>
<pre class="r"><code>library(tidyverse)
# Ensure reproducibility by setting random number seed
set.seed(123) 
# 
x &lt;- rnorm(50,mean=100,sd=10)
# y = 0.8 * x + noise
y &lt;- 0.8 * x + rnorm(50,mean=0,sd=20)

#combine x and y into a single data frame for easy use:
data &lt;- data.frame(x,y)</code></pre>
<p>This data has a true underlying deterministic relationship between x and y, and then I have added some random noise to these data so that all the points don’t lie on a straight line.</p>
<p>We can easily plot these data using ggplot2:</p>
<pre class="r"><code>ggplot(data) +
  geom_point(aes(x=x,y=y),colour=&quot;grey50&quot;)+
  coord_equal() +
  theme_minimal() +
  theme(panel.grid=element_blank(),
        axis.title=element_blank())</code></pre>
<p><img src="/post/2019-03-27-scatter-plots-and-best-fit-lines_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Now lets draw the different lines onto the data.</p>
<p>Lets start with the standard “best fit” line, the regression line of prediction y from x:</p>
<pre class="r"><code>#fit the model
line1 &lt;- lm(y~x)$coef
#extract the slope from the fitted model
line1.slope &lt;- line1[2]
#extract the intercept from the fitted model
line1.intercept &lt;- line1[1]

ggplot(data) +
  geom_point(aes(x=x,y=y),colour=&quot;grey50&quot;)+
  geom_abline(aes(slope=line1.slope,intercept=line1.intercept),colour=&quot;#F8766D&quot;)+ # draw the regression line
  coord_equal() +
  theme_minimal() +
  theme(panel.grid=element_blank(),
        axis.title=element_blank())</code></pre>
<p><img src="/post/2019-03-27-scatter-plots-and-best-fit-lines_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="what-is-the-standard-regression-line-doing" class="section level4">
<h4>What is the standard regression line doing?</h4>
<p>When you perform a standard linear regression analysis and predict a variable y from another variable x, you are making some assumptions about the relationship between the two variables which you might forget about in the context of best fit lines.</p>
<p>The linear model being fit by the regression analysis can be expressed as:</p>
<pre><code>y = Bx + intercept + error</code></pre>
<p>The key assumption here is that there is a simple linear relationship between x and y, whereby each unit increase in x results in a corresponding increase of B in y. Any variation in the data not captured by this relationship is assumed to be the result of random variation. Within most regression analyses this error is assumed to be normally distributed with mean 0 and constant variance across all the values of x (i.e. we predict the data equally well at any point on the line).</p>
<p>A real example of this comes from the work of Francis Galton which helped to give regression analysis it’s name. Galton studied the relationship between the heights of parents and their children. He noted that though taller parents have taller children, those children are less exceptionally tall than their parents. That is, heights “regression to the mean”.</p>
<p>This comes down to the random part of the model which isn’t captured by the coefficients in the model. When someone is exceptionally tall, the model says that this is partly due to underlying factors (e.g. genetics) but also partly driven by random chance. In other words, a very tall parent may pass on the genetic traits for being tall but not the specific combination of other random factors which led to their exceptional height - luck is not inheritted!</p>
<p>Effects like this can be seen in all sorts of fields, for example Sports (see the “Sports Illustrated Cover Curse”), company performance and performance of fund managers. When you measure over time, you tend to see that the best performers in one time period tend to do worse in subsequent periods (and vice versa, the worst performers will tend to do better than next year). One aspect of their performance is down to underlying skills / fundamentals but what makes them exceptional in one year is the extra element of good or bad luck.</p>
<ul>
<li>Reference the stuff from Gelman/Hill here</li>
</ul>
<p>[<a href="https://en.wikipedia.org/wiki/Linear_regression#/media/File:Galton%27s_correlation_diagram_1875.jpg" class="uri">https://en.wikipedia.org/wiki/Linear_regression#/media/File:Galton%27s_correlation_diagram_1875.jpg</a>]</p>
<p><strong>References</strong></p>
<ul>
<li><a href="https://benediktehinger.de/blog/science/scatterplots-regression-lines-and-the-first-principal-component/" class="uri">https://benediktehinger.de/blog/science/scatterplots-regression-lines-and-the-first-principal-component/</a></li>
<li><a href="https://onunicornsandgenes.blog/2013/05/31/how-to-draw-the-line-with-ggplot2/" class="uri">https://onunicornsandgenes.blog/2013/05/31/how-to-draw-the-line-with-ggplot2/</a></li>
<li>Gelman &amp; Hill “Data Analysis Using Regression and Multilevel/Hierarchcial Models” - Section 4.3</li>
</ul>
</div>
