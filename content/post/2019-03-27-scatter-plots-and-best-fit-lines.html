---
title: Scatter plots and best fit lines
author: Steve Burr
date: '2019-03-27'
slug: scatter-plots-and-best-fit-lines
categories:
  - Statistics
  - R
  - DataViz
  - ggplot2
tags: []
---



<p>Last week I came across an interesting conversation on twitter, which raiased something I hadn’t thought about before:</p>
{{% tweet "1107680006476578818" %}}
<p>There were a number of good responses, including links to a few blog posts digging into how all this works, but I wanted to try and summarise what I think are the most important points and have a go at putting together my own versions of the visualisations.</p>
<p>In short, if I have a scatter plot of same data, there are at least three different ways to put a line of best fit through the data -</p>
<p>This post explores the how, why and interpretation of these three lines.</p>
<div id="the-basics" class="section level4">
<h4>The basics</h4>
<p>We’re going to need some simulated data to plot, so let’s simulate some using R:</p>
<pre class="r"><code>library(tidyverse)
# Ensure reproducibility by setting random number seed
set.seed(123) 
# 
x &lt;- rnorm(50,mean=100,sd=10)
# y = 0.8 * x + noise
y &lt;- 0.8 * x + rnorm(50,mean=0,sd=20)

#combine x and y into a single data frame for easy use:
data &lt;- data.frame(x,y)</code></pre>
<p>This data has a true underlying deterministic relationship between x and y, and then I have added some random noise to these data so that all the points don’t lie on a straight line.</p>
<p>We can easily plot these data using ggplot2:</p>
<pre class="r"><code>ggplot(data) +
  geom_point(aes(x=x,y=y),colour=&quot;grey50&quot;)+
  coord_equal() +
  theme_minimal() +
  theme(panel.grid=element_blank(),
        axis.title=element_blank())</code></pre>
<p><img src="/post/2019-03-27-scatter-plots-and-best-fit-lines_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="the-linear-regression-line" class="section level4">
<h4>The linear regression line</h4>
<p>Now lets draw the different lines onto the data.</p>
<p>Lets start with the standard “best fit” line, the regression line of prediction y from x:</p>
<pre class="r"><code>#fit the model
line1 &lt;- lm(y~x)$coef
#extract the slope from the fitted model
line1.slope &lt;- line1[2]
#extract the intercept from the fitted model
line1.intercept &lt;- line1[1]

ggplot(data) +
  geom_point(aes(x=x,y=y),colour=&quot;grey50&quot;)+
  geom_abline(aes(slope=line1.slope,intercept=line1.intercept),colour=&quot;#F8766D&quot;)+ # draw the regression line
  coord_equal() +
  theme_minimal() +
  theme(panel.grid=element_blank(),
        axis.title=element_blank())</code></pre>
<p><img src="/post/2019-03-27-scatter-plots-and-best-fit-lines_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="what-is-the-standard-regression-line-doing" class="section level4">
<h4>What is the standard regression line doing?</h4>
<p>When you perform a standard linear regression analysis and predict a variable y from another variable x, you are making some assumptions about the relationship between the two variables which you might forget about in the context of best fit lines.</p>
<p>The linear model being fit by the regression analysis can be expressed as:</p>
<pre><code>y = Bx + intercept + error</code></pre>
<p>The key assumption here is that there is a simple linear relationship between x and y, whereby each unit increase in x results in a corresponding increase of B in y. Any variation in the data not captured by this relationship is assumed to be the result of random variation. Within most regression analyses this error is assumed to be normally distributed with mean 0 and constant variance across all the values of x (i.e. we predict the data equally well at any point on the line).</p>
<p>A real example of this comes from the work of Francis Galton which helped to give regression analysis its name. Galton studied the relationship between the heights of parents and their children. He noted that though taller parents have taller children, those children are less exceptionally tall than their parents. That is, heights “regress to the mean”.</p>
<div class="figure">
<img src="/post/2019-03-27-scatter-plots-and-best-fit-lines_files/Galton&#39;s_correlation_diagram_1875.jpg" alt="Galton’s linear regression analysis of parent and child heights - Source: Wikipedia" />
<p class="caption">Galton’s linear regression analysis of parent and child heights - Source: Wikipedia</p>
</div>
<p>This comes down to the random part of the model which isn’t captured by the coefficients. When someone is exceptionally tall, the model says that this is partly due to underlying factors (e.g. genetics) but also partly driven by random chance. In other words, a very tall parent may pass on the genetic traits for being tall but not the specific combination of other random factors which led to their exceptional height - luck is not inheritted!</p>
<p>Effects like this can be seen in all sorts of fields, for example Sports (see the “Sports Illustrated Cover Curse”), company performance and performance of fund managers. When you measure over time, you tend to see that the best performers in one time period tend to do worse in subsequent periods (and vice versa, the worst performers will tend to do better than next year). One aspect of their performance is down to underlying skills / fundamentals but what makes them exceptional in one year is the extra element of good or bad luck.</p>
<p>These examples help to give some real world intuition to this concept (and help you reason about the future from what you’ve observed in the past) but we can look at this in more abstract terms. This is well explained in Gelman/Hill’s regression analysis book, and I’ve tried to illustrate their example in the next visualisation:</p>
<ul>
<li>Reference the stuff from Gelman/Hill here</li>
</ul>
<p>Another, final, way of thinking about this is that linear regression is estimating the mean value of y conditional on the value of x. In other words, if we have a lot of data for a given problem, and we want to predict a new value y based on a fixed value of x, then the best thing to do is predict the new y value to be the average of all the data points with the same value of x. In most real cases, we won’t have enough data to just consider x values with the same value as what we want to predict, so the regression analysis allows us to make use of all the data we have across all the values of x to make a prediction at specified value of x.</p>
<p>We can satisfy ourselves that the linear regression line represents the average value of y given a value of x by doing some summarisation. If we split x into five approximately equal chunks, calculate the average value of y within each of these chunks and then add these averages to the scatter plot we see that the linear regression line comes very close to these conditional means:</p>
<p>Draw the regression line vs binned conditional means</p>
<p><strong>References</strong></p>
<ul>
<li><a href="https://benediktehinger.de/blog/science/scatterplots-regression-lines-and-the-first-principal-component/" class="uri">https://benediktehinger.de/blog/science/scatterplots-regression-lines-and-the-first-principal-component/</a></li>
<li><a href="https://onunicornsandgenes.blog/2013/05/31/how-to-draw-the-line-with-ggplot2/" class="uri">https://onunicornsandgenes.blog/2013/05/31/how-to-draw-the-line-with-ggplot2/</a></li>
<li>Gelman &amp; Hill “Data Analysis Using Regression and Multilevel/Hierarchcial Models” - Section 4.3</li>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression" class="uri">https://en.wikipedia.org/wiki/Linear_regression</a></li>
</ul>
</div>
